{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, time, zipfile\n",
    "import tensorflow as tf\n",
    "\n",
    "bert_path = '../../dataset/bert/'\n",
    "sys.path.append(os.path.abspath(os.path.dirname(bert_path)))\n",
    "import modeling\n",
    "import extract_features\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = '../../dataset/origin/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we feed BERT the data from these three files.\n",
    "For each line, we want to obtain contextual embeddings for the 3 target words (A, B, Pronoun).\n",
    "Here are some helper functions to keep track of the offsets of the target words.\n",
    "'''\n",
    "def compute_offset_no_spaces(text, offset):\n",
    "    count = 0\n",
    "    for pos in range(offset):\n",
    "        if text[pos] != \" \": count +=1\n",
    "    return count\n",
    "\n",
    "def count_chars_no_special(text):\n",
    "    count = 0\n",
    "    special_char_list = [\"#\"]\n",
    "    for pos in range(len(text)):\n",
    "        if text[pos] not in special_char_list: count+=1\n",
    "    return count\n",
    "\n",
    "def count_length_no_special(text):\n",
    "    count=0\n",
    "    special_char_list = [\"#\", \" \"]\n",
    "    for pos in range(len(text)):\n",
    "        if text[pos] not in special_char_list: count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the data from a file passes it through BERT to obtain contextual embeddings for the target words,\n",
    "then returns these embeddings in the emb DataFrame.\n",
    "Below, we will use it 3 times,\n",
    "once for each of the files gap-test, gap-development, gap-validation\n",
    "'''\n",
    "path = '../../dataset/bert/'\n",
    "def run_bert(data):\n",
    "    text = data['Text']\n",
    "    text.to_csv('input.txt', index=False, header=False)\n",
    "    os.system(\"python extract_features.py\\\n",
    "    --input_file=input.txt\\\n",
    "    --output_file=output.jsonl\\\n",
    "    --vocab_file=\"+path+\"vocab.txt\\\n",
    "    --bert_config_file=\"+path+\"bert_config.json\\\n",
    "    --init_checkpoint=\"+path+\"bert_model.ckpt\\\n",
    "    --layers=-1\\\n",
    "    --max_seq_length=256\\\n",
    "    --batch_size=8\")\n",
    "    bert_output = pd.read_json(\"output.jsonl\", lines=True)\n",
    "    os.system(\"rm output.jsonl\")\n",
    "    os.system(\"rm input.txt\")\n",
    "    index = data.index\n",
    "    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n",
    "    emb = pd.DataFrame(index=index, columns=columns)\n",
    "    emb.index.name = \"ID\"\n",
    "    for i in range(len(data)):\n",
    "        P = data.loc[i,\"Pronoun\"].lower()\n",
    "        A = data.loc[i, \"A\"].lower()\n",
    "        B = data.loc[i, \"B\"].lower()\n",
    "        \n",
    "        P_offset = compute_offset_no_spaces(data.loc[i, \"Text\"], data.loc[i, \"Pronoun-offset\"])\n",
    "        A_offset = compute_offset_no_spaces(data.loc[i, \"Text\"], data.loc[i, \"A-offset\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
