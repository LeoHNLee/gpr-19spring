## sources

- [kaggle w/ fe](https://www.kaggle.com/keyit92/coref-by-mlp-cnn-coattention)
- [machine reading comprehension - naver d2 seminar](https://www.youtube.com/watch?v=XBCkJck0cdY)
- [reading text & machine QA - naver d2 seminar](https://www.youtube.com/watch?v=r0veZ_WV0sA)

preparing
---------

### reference

-	pass

word flow
=========

word level embedding
--------------------

### word2Vec

-	CBOW
-	skip-gram

### word embedding

-	glove
-	fasttext

character level embedding
-------------------------

-	RNN ~ CNN 성능 비슷
-	속도 : RNN<CNN

Contextual Embedding
--------------------

-	Cove
-	ElMo

attention
---------

-	연관성이 높은 단어들을 맞춰주는 내용

### self-attention

-	스스로와 attention

feature selection
-----------------

-	mention pair
	-	mention : her, his
	-	선행사 : 실제 이름
-	features
	-	pronoun이 들어간 문장의 헤더
	-	pronoun과 정답과의 거리
	-	dependency words
-	mention pair encoder : 가능한 mention pair들을 전부 찾아서, 각 pair들의 점수를 리턴해준다.
-	clustering ranking model

감정분석
--------

### model

-	bag of words 기반
	-	fasttext
-	deep-learning
	-	att bi-lstm

### data collection

-	twitter
