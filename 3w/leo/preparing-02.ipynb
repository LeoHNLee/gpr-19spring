{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3w preparing\n",
    "\n",
    "## pre\n",
    "\n",
    "1. regularization : 소문자/대문자 변환 : 전체에서\n",
    "4. regularization : 약어 대체 : 전체에서 \n",
    "1. tokenization : 텍스트 -> 문장\n",
    "2. tokenization : 문자 -> 단어\n",
    "1. regularization : 문장 부호 제거 : 단어 단위에서\n",
    "3. regularization : 불용어 처리 : 단어 단위에서\n",
    "5. regularization : 반복되는 문자 처리 : 단어 단위에서\n",
    "\n",
    "## 작업 순서\n",
    "\n",
    "- num to word : don't do that, cuz' too many\n",
    "- stop words control\n",
    "- save to np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, warnings\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append(os.path.abspath(os.path.dirname('../../modules/')))\n",
    "from replacers import RegexpReplacer, RepeatReplacer, WordReplacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constants.py',\n",
       " 'CONTRIBUTING.md',\n",
       " 'gap-development.tsv',\n",
       " 'gap-test.tsv',\n",
       " 'gap-validation.tsv',\n",
       " 'gap_scorer.py',\n",
       " 'LICENSE',\n",
       " 'README.md']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_github = '../../dataset/github/'\n",
    "os.listdir(path_github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(path_github+'gap-test.tsv', sep='\\t')\n",
    "validation = pd.read_csv(path_github+'gap-validation.tsv', sep='\\t')\n",
    "train = pd.read_csv(path_github+'gap-development.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>validation-1</td>\n",
       "      <td>He admitted making four trips to China and pla...</td>\n",
       "      <td>him</td>\n",
       "      <td>256</td>\n",
       "      <td>Jose de Venecia Jr</td>\n",
       "      <td>208</td>\n",
       "      <td>False</td>\n",
       "      <td>Abalos</td>\n",
       "      <td>241</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Commission_on_Ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>validation-2</td>\n",
       "      <td>Kathleen Nott was born in Camberwell, London. ...</td>\n",
       "      <td>She</td>\n",
       "      <td>185</td>\n",
       "      <td>Ellen</td>\n",
       "      <td>110</td>\n",
       "      <td>False</td>\n",
       "      <td>Kathleen</td>\n",
       "      <td>150</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kathleen_Nott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>validation-3</td>\n",
       "      <td>When she returns to her hotel room, a Liberian...</td>\n",
       "      <td>his</td>\n",
       "      <td>435</td>\n",
       "      <td>Jason Scott Lee</td>\n",
       "      <td>383</td>\n",
       "      <td>False</td>\n",
       "      <td>Danny</td>\n",
       "      <td>406</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Hawaii_Five-0_(20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>validation-4</td>\n",
       "      <td>On 19 March 2007, during a campaign appearance...</td>\n",
       "      <td>he</td>\n",
       "      <td>333</td>\n",
       "      <td>Reucassel</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>Debnam</td>\n",
       "      <td>325</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Craig_Reucassel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>validation-5</td>\n",
       "      <td>By this time, Karen Blixen had separated from ...</td>\n",
       "      <td>she</td>\n",
       "      <td>427</td>\n",
       "      <td>Finch Hatton</td>\n",
       "      <td>290</td>\n",
       "      <td>False</td>\n",
       "      <td>Beryl Markham</td>\n",
       "      <td>328</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Denys_Finch_Hatton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               Text Pronoun  \\\n",
       "0  validation-1  He admitted making four trips to China and pla...     him   \n",
       "1  validation-2  Kathleen Nott was born in Camberwell, London. ...     She   \n",
       "2  validation-3  When she returns to her hotel room, a Liberian...     his   \n",
       "3  validation-4  On 19 March 2007, during a campaign appearance...      he   \n",
       "4  validation-5  By this time, Karen Blixen had separated from ...     she   \n",
       "\n",
       "   Pronoun-offset                   A  A-offset  A-coref              B  \\\n",
       "0             256  Jose de Venecia Jr       208    False         Abalos   \n",
       "1             185               Ellen       110    False       Kathleen   \n",
       "2             435     Jason Scott Lee       383    False          Danny   \n",
       "3             333           Reucassel       300     True         Debnam   \n",
       "4             427        Finch Hatton       290    False  Beryl Markham   \n",
       "\n",
       "   B-offset  B-coref                                                URL  \n",
       "0       241    False  http://en.wikipedia.org/wiki/Commission_on_Ele...  \n",
       "1       150     True         http://en.wikipedia.org/wiki/Kathleen_Nott  \n",
       "2       406     True  http://en.wikipedia.org/wiki/Hawaii_Five-0_(20...  \n",
       "3       325    False       http://en.wikipedia.org/wiki/Craig_Reucassel  \n",
       "4       328     True    http://en.wikipedia.org/wiki/Denys_Finch_Hatton  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대/소문자\n",
    "def lower_casing(x):\n",
    "    return x.lower()\n",
    "\n",
    "# 약어 대체\n",
    "reg_exp_replacer = RegexpReplacer()\n",
    "\n",
    "# 단어 토큰화\n",
    "def word_tokenizing(doc): # doc 단위로\n",
    "    return map(nltk.word_tokenize, doc) # sent 단위로\n",
    "\n",
    "# 문장 부호 제거\n",
    "punctuation_replacer = re.compile('[%s]'%re.escape(string.punctuation))\n",
    "def punctuation_replacing(doc):\n",
    "    ret = []\n",
    "    for sent in doc:\n",
    "        sent_ret = []\n",
    "        for word in sent:\n",
    "            temp = punctuation_replacer.sub(u'', word)\n",
    "            if not temp == u'': \n",
    "                sent_ret.append(temp)\n",
    "        ret.append(sent_ret)\n",
    "    return ret\n",
    "    \n",
    "# 불용어 제거\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def stop_words_replacing(doc):\n",
    "    ret = []\n",
    "    for sent in doc:\n",
    "        ret.append([word for word in sent if word not in stop_words])\n",
    "    return ret\n",
    "\n",
    "# 반복되는 문자 처리\n",
    "repeat_replacer = RepeatReplacer()\n",
    "def repeat_replacing(doc):\n",
    "    ret=[]\n",
    "    for sent in doc:\n",
    "        sent_ret = []\n",
    "        for word in sent:\n",
    "            sent_ret.append(repeat_replacer.replace(word))\n",
    "        ret.append(sent_ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_names = ['train', 'test', 'validation']\n",
    "for i, df in enumerate((train, test, validation,)):\n",
    "    docs = df['Text'].values.tolist()\n",
    "    for foo in (lower_casing, reg_exp_replacer.replace, sent_tokenize, word_tokenizing, punctuation_replacing,repeat_replacing):\n",
    "        docs = list(map(foo, docs))\n",
    "    np.save('../../dataset/prepared/'+file_names[i], np.array(docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
